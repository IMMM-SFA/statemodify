{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c1bdf6-ceb5-4ae8-8bbb-2922f512484d",
   "metadata": {},
   "source": [
    "## `statemodify` Quickstarter Notebook #4: Using External Methods to Generate Synthetic Streamflow Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e986c-7a52-4e57-8966-70ec70278513",
   "metadata": {},
   "source": [
    "In this notebook, we will demonstrate Option 2 that is provided by `statemodify`, which is the ability to create future states of the world through external methods rather than manipulating the baseline time series as we do in Option 1. Here we offer a method to generate alternative streamflow time series for the five West Slope basins using the `modify_xbm_iwr()` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8b446-bb33-45be-8c4d-d886ed2ebb06",
   "metadata": {},
   "source": [
    "The Colorado River Basin (CRB) is experiencing a shift to a more arid climate which will likely be characterized by droughts that are longer and more severe than what has been experienced historically. The historic streamflow record is now not a sufficient representation of future hydroclimate. Thus, we need methods of creating plausible future streamflow scenarios for the region that can plausibly expand this historic record.\n",
    "\n",
    "In this notebook, we demonstrate how to use the multi-site Hidden Markov model (HMM)-based synthetic streamflow generator in `statemodify` to create ensembles of plausible future regional streamflow traces for our five West Slope basins. The HMM is fit to log annual historical streamflow across the outlet gauges of five key basins on the West Slope of the state of Colorado that drain into the Colorado River. The model can then be used to generate streamflow scenarios that envelope the historical record and expand the representation of natural variability in the hydroclimate while still preserving the inter-site correlations across the outlet gauges and at sites within each of the five basins. The HMM-based model is particularly useful for generating sequences of streamflow that exhibit long persistence, including decadal to multidecadal hydrological drought conditions that are scarcely represented within the historical records. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68df65-c640-4615-b41a-c1ca57380a82",
   "metadata": {},
   "source": [
    "By fitting the HMM on the historical time series of the basins, we create synthetic traces that are stationary and thus give an indication of the extent of natural variability that characterizes the region. The HMM can be extended to create non-stationary traces, as shown in Hadjimichael et al. (2020), but that version of the model is not currently included in `statemodify`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6f259-5b24-49e7-9ab1-ad26841fe200",
   "metadata": {},
   "source": [
    "### Step 1: Fit Multi-Site HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc93cd-8db4-4884-9bae-22388fe71724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "from string import Template\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statemodify as stm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeaa9da-c56c-438c-8763-07deebd5a067",
   "metadata": {},
   "source": [
    "First we define directories and associated paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688cee15-bf70-47e9-ab13-e760476c6f5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# statemod directory\n",
    "statemod_dir = \"/usr/src/statemodify/statemod_upper_co\"\n",
    "\n",
    "# root directory of statemod data for the target basin\n",
    "root_dir = os.path.join(statemod_dir, \"src\", \"main\", \"fortran\")\n",
    "\n",
    "# home directory of notebook instance\n",
    "home_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# path to the statemod executable\n",
    "statemod_exe = os.path.join(root_dir, \"statemod\")\n",
    "\n",
    "# data directory and root name for the target basin\n",
    "data_dir = os.path.join(home_dir, \"data\", \"cm2015_StateMod\", \"StateMod\")\n",
    "\n",
    "# directory to the target basin input files with root name for the basin\n",
    "basin_path = os.path.join(data_dir, \"cm2015B\")\n",
    "\n",
    "# scenarios output directory\n",
    "scenarios_dir = os.path.join(data_dir, \"scenarios\")\n",
    "\n",
    "# path to iwr/xbm file\n",
    "xbm_iwr_template_file = os.path.join(home_dir, \"data\", \"cm2015B_template_xbm_iwr.rsp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00c4383-689e-4c2d-b732-db2085009165",
   "metadata": {},
   "source": [
    "We create a function `hmm_multisite_fit()` that houses the Python code to fit the multi-site HMM to annual flow at the outlet gauges of the five basins. In this function, you can specify the output directory to store the HMM parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce17caa-a645-40c8-8bc0-3a35b3034f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make directory to store HMM parameters\n",
    "\n",
    "output_dir = os.path.join(data_dir, \"HMM_parameters\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "n_basins = 5\n",
    "\n",
    "# choice to save parameters to NumPy array files\n",
    "save_parameters = True\n",
    "\n",
    "fit_array_dict = stm.hmm_multisite_fit(\n",
    "    n_basins=n_basins, save_parameters=save_parameters, output_directory=output_dir\n",
    ")\n",
    "\n",
    "# unpack output dictionary\n",
    "unconditional_dry = fit_array_dict[\"unconditional_dry\"]\n",
    "unconditional_wet = fit_array_dict[\"unconditional_wet\"]\n",
    "logAnnualQ_h = fit_array_dict[\"logAnnualQ_h\"]\n",
    "transition_matrix = fit_array_dict[\"transition_matrix\"]\n",
    "covariance_matrix_wet = fit_array_dict[\"covariance_matrix_wet\"]\n",
    "covariance_matrix_dry = fit_array_dict[\"covariance_matrix_dry\"]\n",
    "wet_state_means = fit_array_dict[\"wet_state_means\"]\n",
    "dry_state_means = fit_array_dict[\"dry_state_means\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a97d7c-b34d-4dfb-ad81-dce665e8cfef",
   "metadata": {},
   "source": [
    "### Step 2: Sample from multi-site HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7cf976-9740-439a-b516-ceb3417a652a",
   "metadata": {},
   "source": [
    "We then use the `hmm_multisite_sample()` function to sample from the HMM 100 times to develop 100 alternative, 105-year traces of streamflow at the outlet gauge of each basin and we save each trace in a .csv file in the `HMM_Runs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7bdd2e-d9f3-47a2-b917-6ee413cee9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a folder to store the runs\n",
    "\n",
    "output_dir = os.path.join(data_dir, \"HMM_Runs\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# using the outputs of the fit function above; this function write output sample files to the output directory\n",
    "stm.hmm_multisite_sample(\n",
    "    logAnnualQ_h,\n",
    "    transition_matrix,\n",
    "    unconditional_dry,\n",
    "    dry_state_means,\n",
    "    wet_state_means,\n",
    "    covariance_matrix_dry,\n",
    "    covariance_matrix_wet,\n",
    "    n_basins=n_basins,\n",
    "    output_directory=output_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fb727-3045-456e-8d47-f60698bf7756",
   "metadata": {},
   "source": [
    "Then one can plot flow duration curves (FDCs) of the annual synthetically generated flow at each basin compared to the the historical record. We see that the HMM is enveloping the historical record and also expanding it around it, particularly around the tails of the distribution, which will lead to more instances of extreme flood and drought events.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26adf0d-f856-4a47-abc3-3dd10ee8a822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stm.plot_flow_duration_curves(\n",
    "    flow_realizations_directory=output_dir,\n",
    "    save_figure=True,\n",
    "    output_directory=output_dir,\n",
    "    figure_name=\"FDC\",\n",
    "    dpi=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22f613-cd4a-42cc-869a-a33896ccbd72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Modify StateMod Input Files for Exploratory Analyses- Streamflow Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa178c4c-dfd6-4e92-b07d-2fecef231212",
   "metadata": {},
   "source": [
    "In order for the HMM to be used in conjunction with StateMod, we utilize a statistical disaggregation technique to disaggregate the synthetically generated outlet gauge flow to the upstream nodes and also from an annual to monthly time scale. The synthetic log-space annual flows are converted to real space and temporally downscaled to monthly flows using a modification of the proportional scaling method used by Nowak et al. (2010). First, a historical year is probabilistically selected based on its “nearness” to the synthetic flow at the last node in terms of annual total. The shifted monthly flows at the last node are then downscaled to all other nodes using the same ratios of monthly flows at the upstream nodes to the last node as in the historically selected year.Though not demonstrated in this notebook, the irrigation demands (in the `.iwr` file) are also inherently tied to the generation of the streamflow, as irrigation demands will increase in dry years. Thus, a regression is fit across historical irrigation anomalies and historical annual flow anomalies and the appropriate irrigation anomaly is determined from this regression for every synthetically generated flow anomaly. More information on this method can be found in Hadjimichael et al., 2020. All of this functionality is embedded in the `modify_xbm_iwr()` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95781b-e5b6-429e-9841-244c16808a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make directory to store input files\n",
    "\n",
    "output_dir = os.path.join(data_dir, \"input_files\")\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "flow_realizations_directory = os.path.join(data_dir, \"HMM_Runs\")\n",
    "\n",
    "scenario = \"1\"\n",
    "\n",
    "# basin name to process\n",
    "basin_name = \"Upper_Colorado\"\n",
    "\n",
    "# seed value for reproducibility if so desired\n",
    "seed_value = 123\n",
    "\n",
    "# number of jobs to launch in parallel; -1 is all but 1 processor used\n",
    "n_jobs = 2\n",
    "\n",
    "# number of samples to generate (how many new xbm and iwr files); produces an IWR multiplier\n",
    "n_samples = 1\n",
    "\n",
    "# generate a batch of files using generated LHS\n",
    "stm.modify_xbm_iwr(\n",
    "    output_dir=output_dir,\n",
    "    flow_realizations_directory=flow_realizations_directory,\n",
    "    scenario=scenario,\n",
    "    basin_name=basin_name,\n",
    "    seed_value=seed_value,\n",
    "    n_jobs=n_jobs,\n",
    "    n_samples=n_samples,\n",
    "    save_sample=True,\n",
    "    randomly_select_flow_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0077776-ead0-48c6-a1c4-6647268f0efa",
   "metadata": {},
   "source": [
    "### Step 4: Read in the New Input Files and Run StateMod : Streamflow Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a86281-2bda-4f2e-8db3-62d28c514142",
   "metadata": {},
   "source": [
    "Now that we have created the new files, the next step is to run them through StateMod. We create a template `.rsp` file (`cm2015B_template_xbm_iwr.rsp`) and swap in the path to the alternative `.xbm` and `.iwr` files that are created. Then we run StateMod for the single scenario and one can then go on and extract shortages or reservoir levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad0e56-03bd-4a8e-93a9-596a27d97df4",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> In order to expedite simulations for the Upper Colorado dataset, make sure to turn off \"Reoperation\" mode. You can do so by opening /home/jovyan/data/cm2015_StateMod/StateMod/cm2015.ctl, navigating to the \"ireopx\" entry and changing the value from \"0\" to \"10\".  </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2e87bb-5676-4a51-ad85-e0b88ab0437d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set realization and sample\n",
    "realization = 1\n",
    "sample = np.arange(0, 1, 1)\n",
    "\n",
    "# read RSP template\n",
    "with open(xbm_iwr_template_file) as template_obj:\n",
    "    # read in file\n",
    "    template_rsp = Template(template_obj.read())\n",
    "\n",
    "    for i in sample:\n",
    "        # create scenario name\n",
    "        scenario = f\"S{i}_{realization}\"\n",
    "\n",
    "        # dictionary holding search keys and replacement values to update the template file\n",
    "        d = {\n",
    "            \"XBM\": f\"../../input_files/cm2015B_{scenario}.xbm\",\n",
    "            \"IWR\": f\"../../input_files/cm2015B_{scenario}.iwr\",\n",
    "        }\n",
    "\n",
    "        # update the template\n",
    "        new_rsp = template_rsp.safe_substitute(d)\n",
    "\n",
    "        # construct simulated scenario directory\n",
    "        simulated_scenario_dir = os.path.join(scenarios_dir, scenario)\n",
    "        if not os.path.exists(simulated_scenario_dir):\n",
    "            os.makedirs(simulated_scenario_dir)\n",
    "\n",
    "        # target rsp file\n",
    "        rsp_file = os.path.join(simulated_scenario_dir, f\"cm2015B_{scenario}.rsp\")\n",
    "\n",
    "        # write updated rsp file\n",
    "        with open(rsp_file, \"w\") as f1:\n",
    "            f1.write(new_rsp)\n",
    "\n",
    "        # construct simulated basin path\n",
    "        simulated_basin_path = f\"cm2015B_{scenario}\"\n",
    "\n",
    "        # run StateMod\n",
    "        print(f\"Running: {scenario}\")\n",
    "        os.chdir(simulated_scenario_dir)\n",
    "\n",
    "        subprocess.call([statemod_exe, simulated_basin_path, \"-simulate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a91d1-7c63-4459-a6f3-2590516c13f3",
   "metadata": {},
   "source": [
    "It's easiest to see the value of generating multiple streamflow scenarios if we run 100-1000 scenarios through StateMod. However, this container does not have the resources to support exploratory modeling at this scale. So we run these simulations externally and below, we read in the `.xre` files with the `read_xre()` helper function and show the distribution of the reservoir levels that are observed across Lake Granby in the Upper Colorado under the 100 simulated scenarios versus the historical 105-year period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852e116-c490-4fac-b8a2-34db27520917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example with Granby Lake\n",
    "zip_file_path = os.path.join(home_dir, \"data\", \"Granby_Dataset.zip\")\n",
    "final_directory = os.path.join(home_dir, \"data/\")\n",
    "\n",
    "!unzip $zip_file_path -d $final_directory\n",
    "granby_hmm, granby_hist, granby_hist_mean, granby_hist_1p = stm.read_xre(\n",
    "    os.path.join(home_dir, \"data/Upper_Colorado/\"), \"Granby\"\n",
    ")\n",
    "\n",
    "# Plot quantiles\n",
    "stm.plot_res_quantiles(granby_hmm, granby_hist_mean, \"Lake Granby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b870294-6e7b-45a0-a086-4188a242909e",
   "metadata": {},
   "source": [
    "Here, we plot the monthly reservoir storage quantiles across each month of the year. The shading corresponds to the larger 100-member sample from the HMM and the dotted black line corresponds to the monthly average storage across the historical 105-year record. Importantly, the HMM is expanding the distribution of reservoir storages, particularly creating both larger and smaller storages, meaning that we are capturing reservoir levels under a broader range of wetter and drier conditions that could have implications for shortages for users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56667fa7-4147-4b22-9f0d-1f0871213526",
   "metadata": {},
   "source": [
    "We can also plot the range of monthly storages from the HMM and historical period as box plots for an alternative comparison using the `plot_reservoir_boxes()` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757bb9c-8c95-4a84-a481-0b878eee6be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using the output of the above `read_xre` function as inputs\n",
    "stm.plot_reservoir_boxes(granby_hmm, granby_hist, \"Lake Granby\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ea969-9f94-4c57-bab1-36b707412588",
   "metadata": {},
   "source": [
    "Here, the blue box plots correspond to the HMM-generated reservoir storages and the orange box plots correspond to the historical monthly dataset. The black circles represent outliers. As illustrated in the quantile plot above as well, for all months, the HMM is creating a wider distribution of reservoir storages, and tends to be able to encompass even historical outliers. Remember that the HMM has only been fit on the historical dataset. Thus, the HMM can provide an estimate of the expanse of reservoir storages that can be expected just within the range of natural variability, which is quite large! Particularly, the HMM is creating many more instances of drier scenarios and lower reservoir levels which can be very useful for informing drought vulnerability assessments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b9bfa-790d-4e2a-9e50-229811da69bf",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Nowak, K., Prairie, J., Rajagopalan, B., & Lall, U. (2010). A nonparametric stochastic approach for multisite disaggregation of annual to daily streamflow. Water resources research, 46(8)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
